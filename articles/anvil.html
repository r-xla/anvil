<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Get Started • anvil</title>
<!-- mathjax math --><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script><script>
  window.MathJax = {
    chtml: {
      fontURL: "https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/output/chtml/fonts/woff-v2"
    }
  };
</script><script src="../lightswitch.js"></script><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Get Started">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top " aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">anvil</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.0.0.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="active nav-item"><a class="nav-link" href="../articles/anvil.html">Get started</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/r-xla/anvil/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-lightswitch" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true" aria-label="Light switch"><span class="fa fa-sun"></span></button>
  <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="dropdown-lightswitch">
<li><button class="dropdown-item" data-bs-theme-value="light"><span class="fa fa-sun"></span> Light</button></li>
    <li><button class="dropdown-item" data-bs-theme-value="dark"><span class="fa fa-moon"></span> Dark</button></li>
    <li><button class="dropdown-item" data-bs-theme-value="auto"><span class="fa fa-adjust"></span> Auto</button></li>
  </ul>
</li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Get Started</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/r-xla/anvil/blob/main/vignettes/anvil.Rmd" class="external-link"><code>vignettes/anvil.Rmd</code></a></small>
      <div class="d-none name"><code>anvil.Rmd</code></div>
    </div>

    
    
<p>In this vignette, you will learn everything you need to know to get
started implementing numerical algorithms using {anvil}. If you have
experience with JAX in Python, you should feel right at home.</p>
<div class="section level2">
<h2 id="the-anviltensor">The <code>AnvilTensor</code><a class="anchor" aria-label="anchor" href="#the-anviltensor"></a>
</h2>
<p>We will start by introducing the main data structure, which is the
<code>AnvilTensor</code>. It is essentially like an <code>R</code>
array, with some differences:</p>
<ol style="list-style-type: decimal">
<li>It supports more data types, such as different precisions, as well
as unsigned integers.</li>
<li>The tensor can live on different <em>platforms</em>, such as CPU or
GPU.</li>
<li>0-dimensional tensors can be used to represent scalars.</li>
</ol>
<p>We can create such an object from R data types using the
<code>nv_tensor</code> and friends functions. Below, we create a
0-dimensional tensor (i.e., a scalar) of type <code>int16</code> on the
CPU.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://r-xla.github.io/anvil/" class="external-link">anvil</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">42</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/nv_tensor.html">nv_tensor</a></span><span class="op">(</span><span class="fl">1L</span>, dtype <span class="op">=</span> <span class="st">"i16"</span>, device <span class="op">=</span> <span class="st">"cpu"</span>, shape <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/integer.html" class="external-link">integer</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## AnvilTensor </span></span>
<span><span class="co">##  1</span></span>
<span><span class="co">## [ CPUi16{} ]</span></span></code></pre>
<p>Note that for creation of scalars, you can also use
<code>nv_scalar</code> as a shorthand to skip specifying the shape.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/nv_tensor.html">nv_scalar</a></span><span class="op">(</span><span class="fl">1L</span>, dtype <span class="op">=</span> <span class="st">"i16"</span>, device <span class="op">=</span> <span class="st">"cpu"</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## AnvilTensor </span></span>
<span><span class="co">##  1</span></span>
<span><span class="co">## [ CPUi16{} ]</span></span></code></pre>
<p>We can also create higher-dimensional tensors, for example a 2x3 CPU
tensor of type <code>f32</code>. Below, we omit specifying the platform
and datatype, as it will default to <code>"cpu"</code> and
<code>"f32"</code>. Note that the default datatype depends on the input
datatype.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/array.html" class="external-link">array</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">6</span>, dim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/nv_tensor.html">nv_tensor</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## AnvilTensor </span></span>
<span><span class="co">##  1 3 5</span></span>
<span><span class="co">##  2 4 6</span></span>
<span><span class="co">## [ CPUi32{2x3} ]</span></span></code></pre>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/nv_tensor.html">nv_tensor</a></span><span class="op">(</span><span class="va">x</span>, dtype <span class="op">=</span> <span class="st">"f32"</span><span class="op">)</span></span>
<span><span class="va">y</span></span></code></pre></div>
<pre><code><span><span class="co">## AnvilTensor </span></span>
<span><span class="co">##  1.0000 3.0000 5.0000</span></span>
<span><span class="co">##  2.0000 4.0000 6.0000</span></span>
<span><span class="co">## [ CPUf32{2x3} ]</span></span></code></pre>
<p>In order to convert an <code>AnvilTensor</code> back to a regular R
array, you can use the <code><a href="https://r-xla.github.io/tengen/reference/as_array.html" class="external-link">as_array()</a></code> function.</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://r-xla.github.io/tengen/reference/as_array.html" class="external-link">as_array</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##      [,1] [,2] [,3]</span></span>
<span><span class="co">## [1,]    1    3    5</span></span>
<span><span class="co">## [2,]    2    4    6</span></span></code></pre>
<p>At first, working with <code>AnvilTensor</code>s may feel a bit
cumbersome, because you cannot directly apply functions to them like you
would with regular R arrays.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">x</span> <span class="op">+</span> <span class="va">x</span></span></code></pre></div>
<pre><code><span><span class="co">##      [,1] [,2] [,3]</span></span>
<span><span class="co">## [1,]    2    6   10</span></span>
<span><span class="co">## [2,]    4    8   12</span></span></code></pre>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#y + y</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="jit-compilation">JIT Compilation<a class="anchor" aria-label="anchor" href="#jit-compilation"></a>
</h2>
<p>In order to work with <code>AnvilTensor</code>s, you need to convert
the function you want to apply to a jit-compiled version via
<code><a href="../reference/jit.html">anvil::jit()</a></code>.</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">plus_jit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/jit.html">jit</a></span><span class="op">(</span><span class="va">`+`</span><span class="op">)</span></span>
<span><span class="fu">plus_jit</span><span class="op">(</span><span class="va">y</span>, <span class="va">y</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## AnvilTensor </span></span>
<span><span class="co">##   2.0000  6.0000 10.0000</span></span>
<span><span class="co">##   4.0000  8.0000 12.0000</span></span>
<span><span class="co">## [ CPUf32{2x3} ]</span></span></code></pre>
<p>The result of the operation is again an <code>AnvilTensor</code>.</p>
<p>We can, of course, jit-compile more complex functions as well.</p>
<p>Below, we define a function that takes in a data matrix
<code>X</code>, a weight vector <code>beta</code> and a scalar bias
<code>b</code>, and computes the linear model output <span class="math inline">\(y = X \times \beta + \alpha\)</span>.</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">linear_model_r</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">X</span>, <span class="va">beta</span>, <span class="va">alpha</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">X</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span> <span class="va">beta</span> <span class="op">+</span> <span class="va">alpha</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="va">linear_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/jit.html">jit</a></span><span class="op">(</span><span class="va">linear_model_r</span><span class="op">)</span></span>
<span></span>
<span><span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/nv_tensor.html">nv_tensor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">6</span><span class="op">)</span>, dtype <span class="op">=</span> <span class="st">"f32"</span>, shape <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">beta</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/nv_tensor.html">nv_tensor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">3</span><span class="op">)</span>, dtype <span class="op">=</span> <span class="st">"f32"</span>, shape <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">3</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">alpha</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/nv_tensor.html">nv_scalar</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span>, dtype <span class="op">=</span> <span class="st">"f32"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">linear_model</span><span class="op">(</span><span class="va">X</span>, <span class="va">beta</span>, <span class="va">alpha</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## AnvilTensor </span></span>
<span><span class="co">##   2.7911</span></span>
<span><span class="co">##  -1.1904</span></span>
<span><span class="co">## [ CPUf32{2x1} ]</span></span></code></pre>
<p>One current restriction of {anvil} is that the function has to be
re-compiled for every unique combination of inputs shapes, data-types,
and platforms.</p>
<p>To demonstrate this, we create a slightly modified version of
<code>linear_model</code>.</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">linear_model2</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/jit.html">jit</a></span><span class="op">(</span><span class="kw">function</span><span class="op">(</span><span class="va">X</span>, <span class="va">beta</span>, <span class="va">alpha</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"compiling ...\n"</span><span class="op">)</span></span>
<span>  <span class="va">X</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span> <span class="va">beta</span> <span class="op">+</span> <span class="va">alpha</span></span>
<span><span class="op">}</span><span class="op">)</span></span></code></pre></div>
<p>To demonstrate this, we create a little helper function that creates
example input data with different numbers of observations:</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">simul_data</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">n</span>, <span class="va">p</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>    X <span class="op">=</span> <span class="fu"><a href="../reference/nv_tensor.html">nv_tensor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="va">n</span> <span class="op">*</span> <span class="va">p</span><span class="op">)</span>, dtype <span class="op">=</span> <span class="st">"f32"</span>, shape <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">n</span>, <span class="va">p</span><span class="op">)</span><span class="op">)</span>,</span>
<span>    beta <span class="op">=</span> <span class="fu"><a href="../reference/nv_tensor.html">nv_tensor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="va">p</span><span class="op">)</span>, dtype <span class="op">=</span> <span class="st">"f32"</span>, shape <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">p</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span>,</span>
<span>    alpha <span class="op">=</span> <span class="fu"><a href="../reference/nv_tensor.html">nv_scalar</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span>, dtype <span class="op">=</span> <span class="st">"f32"</span><span class="op">)</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>Below, we call the function twice on data with the same shapes.</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/do.call.html" class="external-link">do.call</a></span><span class="op">(</span><span class="va">linear_model2</span>, <span class="fu">simul_data</span><span class="op">(</span><span class="fl">2</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## compiling ...</span></span></code></pre>
<pre><code><span><span class="co">## AnvilTensor </span></span>
<span><span class="co">##   4.9640</span></span>
<span><span class="co">##  -0.1413</span></span>
<span><span class="co">## [ CPUf32{2x1} ]</span></span></code></pre>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/do.call.html" class="external-link">do.call</a></span><span class="op">(</span><span class="va">linear_model2</span>, <span class="fu">simul_data</span><span class="op">(</span><span class="fl">2</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## AnvilTensor </span></span>
<span><span class="co">##   0.6140</span></span>
<span><span class="co">##  -2.5214</span></span>
<span><span class="co">## [ CPUf32{2x1} ]</span></span></code></pre>
<p>We can notice that we only see the <code>"compiling ..."</code>
message the first and third time. This is, because the first time, the
function is compiled into an <code>XLA</code> executable and cached for
later reuse. The second time, we don’t execute the R function at all,
but directly run the cached <code>XLA</code> executable. This executable
does not contain “standard” R code, like the <code><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat()</a></code> call,
but records only the operations applied to
<code>AnvilTensor</code>s.</p>
<p>If we now call the function on data with different shapes, we see
that the function is re-compiled.</p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">y_hat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/do.call.html" class="external-link">do.call</a></span><span class="op">(</span><span class="va">linear_model2</span>, <span class="fu">simul_data</span><span class="op">(</span><span class="fl">4</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## compiling ...</span></span></code></pre>
<p>Because the compilation step itself can take some time, {anvil}
therefore gives the best results when the same function is called many
times with the same input shapes, data types, and platforms, or the
computation itself is sufficiently large to amortize the compilation
overhead. One common application scenario where this assumption holds
are iterative optimization algorithms.</p>
<div class="section level3">
<h3 id="static-arguments">Static Arguments<a class="anchor" aria-label="anchor" href="#static-arguments"></a>
</h3>
<p>One feature of {anvil} is that not all arguments of
<code>jit</code>-compiled functions need to be
<code>AnvilTensor</code>s. For example, we might want a linear model
with or without an intercept term. To do so, we add the logical argument
<code>with_bias</code> to our function. We need to mark this argument as
<code>static</code>, so {anvil} knows to treat this as a regular R value
instead of an <code>AnvilTensor</code>.</p>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">linear_model3</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/jit.html">jit</a></span><span class="op">(</span><span class="kw">function</span><span class="op">(</span><span class="va">X</span>, <span class="va">beta</span>, <span class="va">alpha</span> <span class="op">=</span> <span class="cn">NULL</span>, <span class="va">with_bias</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="va">with_bias</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"Compiling without bias ...\n"</span><span class="op">)</span></span>
<span>    <span class="va">X</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span> <span class="va">beta</span> <span class="op">+</span> <span class="va">alpha</span></span>
<span>  <span class="op">}</span> <span class="kw">else</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"Compiling with bias ...\n"</span><span class="op">)</span></span>
<span>    <span class="va">X</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span> <span class="va">beta</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">}</span>, static <span class="op">=</span> <span class="st">"with_bias"</span><span class="op">)</span></span></code></pre></div>
<p>We can call this function now with or without a bias term:</p>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">linear_model3</span><span class="op">(</span><span class="va">X</span>, <span class="va">beta</span>, with_bias <span class="op">=</span>  <span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Compiling with bias ...</span></span></code></pre>
<pre><code><span><span class="co">## AnvilTensor </span></span>
<span><span class="co">##   2.8538</span></span>
<span><span class="co">##  -1.1277</span></span>
<span><span class="co">## [ CPUf32{2x1} ]</span></span></code></pre>
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">linear_model3</span><span class="op">(</span><span class="va">X</span>, <span class="va">beta</span>, <span class="va">alpha</span>, with_bias <span class="op">=</span>  <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Compiling without bias ...</span></span></code></pre>
<pre><code><span><span class="co">## AnvilTensor </span></span>
<span><span class="co">##   2.7911</span></span>
<span><span class="co">##  -1.1904</span></span>
<span><span class="co">## [ CPUf32{2x1} ]</span></span></code></pre>
</div>
<div class="section level3">
<h3 id="nested-inputs-and-outputs">Nested Inputs and Outputs<a class="anchor" aria-label="anchor" href="#nested-inputs-and-outputs"></a>
</h3>
<p>Static arguments work differently than <code>AnvilTensors</code> as
the function will not be re-compiled for each new observed value of the
static argument.</p>
<p>Note also, that the inputs, as well as the outputs, can also contain
nested data structures that contain <code>AnvilTensor</code>s, although
we currently only support (named) lists.</p>
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">linear_model4</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/jit.html">jit</a></span><span class="op">(</span><span class="kw">function</span><span class="op">(</span><span class="va">inputs</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>y_hat <span class="op">=</span> <span class="va">inputs</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span> <span class="va">inputs</span><span class="op">[[</span><span class="fl">2</span><span class="op">]</span><span class="op">]</span> <span class="op">+</span> <span class="va">inputs</span><span class="op">[[</span><span class="fl">3</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="op">}</span><span class="op">)</span></span>
<span><span class="fu">linear_model4</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">X</span>, <span class="va">beta</span>, <span class="va">alpha</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## $y_hat</span></span>
<span><span class="co">## AnvilTensor </span></span>
<span><span class="co">##   2.7911</span></span>
<span><span class="co">##  -1.1904</span></span>
<span><span class="co">## [ CPUf32{2x1} ]</span></span></code></pre>
<p>So far, we have only implemented the prediction step for a linear
model. One of the core applications of anvil is to implement learning
algorithms, for which we often need gradients, as well as control flow.
We will start with gradients.</p>
</div>
</div>
<div class="section level2">
<h2 id="automatic-differentiation">Automatic Differentiation<a class="anchor" aria-label="anchor" href="#automatic-differentiation"></a>
</h2>
<p>In anvil, you can easily obtain the gradient function of a
scalar-valued function using <code><a href="../reference/gradient.html">gradient()</a></code>: Currently,
vector-valued functions cannot be differentiated. Below, we implement
implement the loss function for our linear model.</p>
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mse</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">y_hat</span>, <span class="va">y</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">y_hat</span> <span class="op">-</span> <span class="va">y</span><span class="op">)</span><span class="op">^</span><span class="fu"><a href="../reference/nv_tensor.html">nv_scalar</a></span><span class="op">(</span><span class="fl">2.0</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>We now need some target variables <code>y</code>, so we simulate some
data from a linear model:</p>
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">beta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">100</span><span class="op">)</span>, ncol <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">alpha</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="va">X</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span> <span class="va">beta</span> <span class="op">+</span> <span class="va">alpha</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">100</span>, sd <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">X</span>, <span class="va">y</span><span class="op">)</span></span></code></pre></div>
<p><img src="anvil_files/figure-html/unnamed-chunk-16-1.png" width="700"></p>
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/nv_tensor.html">nv_tensor</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/nv_tensor.html">nv_tensor</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span></span></code></pre></div>
<p>Next, we randomly initialize the model parameters:</p>
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">beta_hat</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/nv_tensor.html">nv_tensor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span>, shape <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">)</span>, dtype <span class="op">=</span> <span class="st">"f32"</span><span class="op">)</span></span>
<span><span class="va">alpha_hat</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/nv_tensor.html">nv_scalar</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span>, dtype <span class="op">=</span> <span class="st">"f32"</span><span class="op">)</span></span></code></pre></div>
<p>We can now define a function that does the prediction and calculates
the loss. Note that we are calling into the original R function that
does the prediction and not its jit-compiled version.</p>
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model_loss</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">X</span>, <span class="va">beta</span>, <span class="va">alpha</span>, <span class="va">y</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">y_hat</span> <span class="op">&lt;-</span> <span class="fu">linear_model_r</span><span class="op">(</span><span class="va">X</span>, <span class="va">beta</span>, <span class="va">alpha</span><span class="op">)</span></span>
<span>  <span class="fu">mse</span><span class="op">(</span><span class="va">y_hat</span>, <span class="va">y</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>Using the <code><a href="../reference/gradient.html">gradient()</a></code> transformation, we can
automatically obtain the gradient function of <code>model_loss</code>
with respect to some of its arguments, which we specify.</p>
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model_loss_grad</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/gradient.html">gradient</a></span><span class="op">(</span></span>
<span>  <span class="va">model_loss</span>,</span>
<span>  wrt <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"beta"</span>, <span class="st">"alpha"</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>Finally, we define the update step for the weights using gradient
descent.</p>
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">update_weights</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/jit.html">jit</a></span><span class="op">(</span><span class="kw">function</span><span class="op">(</span><span class="va">X</span>, <span class="va">beta</span>, <span class="va">alpha</span>, <span class="va">y</span>, <span class="va">lr</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">lr</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/nv_tensor.html">nv_scalar</a></span><span class="op">(</span><span class="fl">0.1</span><span class="op">)</span></span>
<span>  <span class="va">grads</span> <span class="op">&lt;-</span> <span class="fu">model_loss_grad</span><span class="op">(</span><span class="va">X</span>, <span class="va">beta</span>, <span class="va">alpha</span>, <span class="va">y</span><span class="op">)</span></span>
<span>  <span class="va">beta_new</span> <span class="op">&lt;-</span> <span class="va">beta</span> <span class="op">-</span> <span class="va">lr</span> <span class="op">*</span> <span class="va">grads</span><span class="op">$</span><span class="va">beta</span></span>
<span>  <span class="va">alpha_new</span> <span class="op">&lt;-</span> <span class="va">alpha</span> <span class="op">-</span> <span class="va">lr</span> <span class="op">*</span> <span class="va">grads</span><span class="op">$</span><span class="va">alpha</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>beta <span class="op">=</span> <span class="va">beta_new</span>, alpha <span class="op">=</span> <span class="va">alpha_new</span><span class="op">)</span></span>
<span><span class="op">}</span><span class="op">)</span></span></code></pre></div>
<p>This already allows us to train our linear model using gradient
descent:</p>
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">weights</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>beta <span class="op">=</span> <span class="va">beta_hat</span>, alpha <span class="op">=</span> <span class="va">alpha_hat</span><span class="op">)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fl">100</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">weights</span> <span class="op">&lt;-</span> <span class="fu">update_weights</span><span class="op">(</span><span class="va">X</span>, <span class="va">weights</span><span class="op">$</span><span class="va">beta</span>, <span class="va">weights</span><span class="op">$</span><span class="va">alpha</span>, <span class="va">y</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] 100</span></span></code></pre>
<p><img src="anvil_files/figure-html/unnamed-chunk-22-1.png" width="700"></p>
<p>While this might seem like a reasonable solution, it continuously
switches between the R interpreter and the XLA runtime. Moreover, we
allocate new tensors in each iteration for the weights. While the latter
might not be a big problem for small models, it can lead to significant
overhead when working with bigger tensors. Next, we will briefly address
the concept of immutability in anvil and which options you have have to
work around it.</p>
</div>
<div class="section level2">
<h2 id="immutability">Immutability<a class="anchor" aria-label="anchor" href="#immutability"></a>
</h2>
<p>Conceptually, whenever we are defining programs in anvil, we are
strictly following <strong>value semantics</strong>. This means,
in-place modifications like updating an array element are conceptually
impossible.</p>
<p>When we are dealing with updating an existing tensor, this might
either be: 1. Updating an <code>AnvilTensor</code> that “lives within” a
jit-compiled function. 2. Updating an <code>AnvilTensor</code> living in
R through a jit-compiled function.</p>
<p>For the first category, there is function TODO demonstrated
below.</p>
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># TODO</span></span></code></pre></div>
<p>The thing to note is that while conceptually, this is <em>not</em> an
in-place update, but creates a new tensor, the XLA compiler is able to
optimize this, ensuring that no unnecessary copies are actually
made.</p>
<p>For the second category, we can mark arguments of a jit-compiled
function as “donatable”. This means, we are telling the XLA runtime that
after we pass those tensors marked as donatable into the function, we
will no longer use them in R. The XLA compiler will therefore be able to
reuse the memory.</p>
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># TODO</span></span></code></pre></div>
<div class="section level3">
<h3 id="control-flow">Control Flow<a class="anchor" aria-label="anchor" href="#control-flow"></a>
</h3>
<p>Earlier, we have already used R control flow to train our linear
model.</p>
<p>In principle, there are three ways to handle control-flow in
anvil:</p>
<ol style="list-style-type: decimal">
<li>Embed jit-compiled functions inside R control-flow constructs, which
we have seen earlier.</li>
<li>Embed R control flow inside a jit-compiled function (we have also
seen this earlier when our linear model allowed to optionally include a
bias term).</li>
<li>Use special control-flow primitives provided by anvil, such as
<code>nv_while()</code> and <code>nv_if()</code>.</li>
</ol>
<p>What’s the best solution depends on the specific scenario.</p>
<p>One thing to be aware of is that we usually don’t want R loops within
the jit-compiled function. This is, because the loop will be unrolled
during compilation, which can lead to very large compilation times and
big executables.</p>
<p>The only thing we have not demonstrated so far is how</p>
<p>When doing this, we have the choice to either implement the loop in
R, and call into a jit-compiled function in each iteration, or implement
the loop as part of the jit-compiled function itself.</p>
<p>The advantage of the former is that this allows including standard R
code in the loop, which can e.g. be useful for logging purposes.
However, depending on the duration of each iteration, there might be a
significant overhead from switching between R and the compiled code.</p>
<p>As an example, we will fit the above linear model using gradient
descent. Of course, we would usually fit the model by solving the normal
equations directly, but this is just for demonstration purposes.</p>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Sebastian Fischer, Daniel Falbel.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer>
</div>





  </body>
</html>
