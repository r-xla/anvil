% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/backward.R
\name{gradient}
\alias{gradient}
\title{Gradient}
\usage{
gradient(f, wrt = NULL)
}
\arguments{
\item{f}{(\code{function})\cr
Function to differentiate. Arguments can be tensorish (\code{\link{AnvilTensor}}) or
static (non-tensor) values. Must return a single scalar float tensor.}

\item{wrt}{(\code{character} or \code{NULL})\cr
Names of the arguments to compute the gradient with respect to.
Only tensorish (float tensor) arguments can be included; static arguments
must not appear in \code{wrt}.
If \code{NULL} (the default), the gradient is computed with respect to all
arguments (which must all be tensorish in that case).}
}
\value{
\code{function}
}
\description{
Returns a new function that computes the gradient of \code{f} via reverse-mode automatic
differentiation. \code{f} must return a single float scalar. The returned function has the
same signature as \code{f} and returns the gradients in the same structure as the inputs
(or the subset selected by \code{wrt}).
}
\examples{
\dontshow{if (pjrt::plugin_is_downloaded()) withAutoprint(\{ # examplesIf}
f <- function(x, y) sum(x * y)
g <- jit(gradient(f))
g(nv_tensor(c(1, 2), dtype = "f32"), nv_tensor(c(3, 4), dtype = "f32"))

# Differentiate with respect to a single argument
g_x <- jit(gradient(f, wrt = "x"))
g_x(nv_tensor(c(1, 2), dtype = "f32"), nv_tensor(c(3, 4), dtype = "f32"))

# Static (non-tensor) arguments are passed through but cannot be in wrt
f2 <- function(x, power) sum(x^power)
g2 <- jit(gradient(f2, wrt = "x"), static = "power")
g2(nv_tensor(c(1, 2, 3), dtype = "f32"), power = 2L)
\dontshow{\}) # examplesIf}
}
\seealso{
\code{\link[=value_and_gradient]{value_and_gradient()}} to get both the output and gradients,
\code{\link[=transform_gradient]{transform_gradient()}} for the low-level graph transformation.
}
