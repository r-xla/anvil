% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/backward.R
\name{gradient}
\alias{gradient}
\alias{value_and_gradient}
\title{Gradient}
\usage{
gradient(f, wrt = NULL)

value_and_gradient(f, wrt = NULL)
}
\arguments{
\item{f}{(\code{function})\cr
Function to compute the gradient of.}

\item{wrt}{(\code{character} or \code{NULL})\cr
Names of the arguments to compute the gradient with respect to.
If \code{NULL} (the default), the gradient is computed with respect to all arguments.}
}
\description{
Transform a function to its gradient.
}
\section{Functions}{
\itemize{
\item \code{value_and_gradient()}: Returns both the value and the gradient

}}
\section{Ambiguity}{

When performing the backward pass, the ambiguity does not really play a role anymore.
This is because we don't call into \code{nv_}-functions anymore that promote non-matching tensors,
but only primitive \code{nvl_}-functions.
The promotion has already be done in the forward pass.
We still want to ensure, however, that the gradient values have the same ambiguity as the input values.
This is simply achieved by setting the ambiguity at the end of the backward pass.
The ambiguity of a single backward rule therefore does not really matter.
}

