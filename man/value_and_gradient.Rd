% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/backward.R
\name{value_and_gradient}
\alias{value_and_gradient}
\title{Value and Gradient}
\usage{
value_and_gradient(f, wrt = NULL)
}
\arguments{
\item{f}{(\code{function})\cr
Function to differentiate. Arguments can be tensorish (\code{\link{AnvilTensor}}) or
static (non-tensor) values. Must return a single scalar float tensor.}

\item{wrt}{(\code{character} or \code{NULL})\cr
Names of the arguments to compute the gradient with respect to.
Only tensorish (float tensor) arguments can be included; static arguments
must not appear in \code{wrt}.
If \code{NULL} (the default), the gradient is computed with respect to all
arguments (which must all be tensorish in that case).}
}
\value{
A function with the same formals as \code{f} that returns
\code{list(value = ..., grad = ...)}.
}
\description{
Returns a new function that computes both the output of \code{f} and its gradient in a
single forward+backward pass. The result is a named list with elements \code{value} (the
original return value of \code{f}) and \code{grad} (the gradients, structured like the inputs or
the \code{wrt} subset).
}
\examples{
\dontshow{if (pjrt::plugin_is_downloaded()) withAutoprint(\{ # examplesIf}
loss_fn <- function(x) sum(x^2L)
vg <- jit(value_and_gradient(loss_fn))
result <- vg(nv_tensor(c(3, 4), dtype = "f32"))
result$value
result$grad
\dontshow{\}) # examplesIf}
}
\seealso{
\code{\link[=gradient]{gradient()}}
}
