nv_unif_rand <- function(
  shape,
  initial_state,
  dtype = "f64"
) {
  checkmate::assertChoice(dtype, c("f32", "f64"))
  shape <- assert_shapevec(shape)

  # 1. Generate random bits (64)
  # 2. We use these as mantissa bits for float, where we set the exponent to 1.0
  # 3. Because we have an implicit leading 1, we get a number in [1, 2) -> need to shift to [0, 1)

  # generate random bits
  # use THREE_FRY as rng algorithm: JAX default
  rbits <- nvl_rng_bit_generator(
    initial_state = initial_state,
    "THREE_FRY",
    paste0("ui", sub("f(\\d+)", "\\1", dtype)),
    shape = shape
  )

  # shift value: 9 for f32, 11 for f64
  shift <- nv_scalar(
    ifelse(dtype == "f32", 9L, 11L),
    dtype = paste0("ui", sub("f(\\d+)", "\\1", dtype))
  )

  # shift to the right, s.t. exponent bits are all 0
  mantissa <- nv_shift_right_logical(rbits[[2]], shift)

  # interpretation of 1.0 (float) as unsigned
  one_bits <- nv_bitcast_convert(
    nv_scalar(1.0, dtype = dtype),
    dtype = paste0("ui", sub("f(\\d+)", "\\1", dtype))
  )

  # bitwise or -> exponent from 1.0 (float), mantissa is random
  U <- nv_or(mantissa, one_bits)

  # convert back to requested dtype
  # resulting RVs  are in [1, 2)
  U <- nv_bitcast_convert(U, dtype = dtype)

  # shift to [0, 1)
  U <- nv_add(U, nv_scalar(-1, dtype = dtype))

  # return state and RVs
  list(rbits[[1]], U)
}

# Random Number Generation API
# This file contains user-facing RNG sampling functions

#' @title Sample from a Uniform Distribution
#' @description
#' Samples from a uniform distribution in the open interval `(lower, upper)`.
#' @template param_shape
#' @template param_initial_state
#' @template param_dtype
#' @param lower,upper (`numeric(1)`)\cr
#'   Lower and upper bound.
#' @return (`list()` of [`tensorish`])\cr
#'   List of two elements: the updated RNG state and the sampled values.
#' @family rng
#' @examplesIf pjrt::plugin_is_downloaded()
#' jit_eval({
#'   state <- nv_rng_state(42L)
#'   result <- nv_runif(c(2, 3), state)
#'   result[[2]]
#' })
#' @export
nv_runif <- function(
  shape,
  initial_state,
  dtype = "f32",
  lower = 0,
  upper = 1
) {
  checkmate::assertChoice(dtype, c("f32", "f64"))
  checkmate::assertNumeric(lower, len = 1, any.missing = FALSE, upper = upper)
  checkmate::assertNumeric(upper, len = 1, any.missing = FALSE, lower = lower)
  shape <- assert_shapevec(shape)

  if (upper == lower) {
    return(nv_broadcast_to(nv_scalar(upper, dtype = dtype), shape = shape))
  }

  .lower <- nv_scalar(lower, dtype = dtype)
  .upper <- nv_scalar(upper, dtype = dtype)
  .range <- nv_sub(.upper, .lower)

  # generate samples in [0, 1)
  Unif <- nv_unif_rand(initial_state = initial_state, shape = shape, dtype = dtype)
  U <- Unif[[2]]

  # check if some values are <= 0
  le_zero <- nv_le(U, nv_scalar(0, dtype = dtype))

  # Define smallest step (like R's 0.5 * i2_32m1 philosophy)
  # for f32 and 23 mantissa bits 2^-24 lies between 0 and 2^-23,
  # the next smallest generated value.
  # Same applies for f64 and 2^-53 and 52 mantissa bits.
  smallest_step <- nv_broadcast_to(
    nv_scalar(
      ifelse(dtype == "f32", 2^-24, 2^-53),
      dtype = dtype
    ),
    shape = shape
  )

  # Replace values <= 0 with smallest_step
  U <- nv_ifelse(le_zero, smallest_step, U)

  # expand to range
  U <- nv_mul(U, .range)
  # shift to interval
  U <- nv_add(U, .lower)

  return(list(Unif[[1]], U))
}

#' @title Sample from a Normal Distribution
#' @description
#' Samples from a normal distribution with mean \eqn{\mu} and standard deviation \eqn{\sigma}
#' using the Box-Muller transform.
#' @template param_shape
#' @template param_initial_state
#' @template param_dtype
#' @param mu ([`tensorish`])\cr
#'   Mean.
#' @param sigma ([`tensorish`])\cr
#'   Standard deviation. Must be positive, otherwise results are invalid.
#' @return (`list()` of [`tensorish`])\cr
#'   List of two elements: the updated RNG state and the sampled values.
#' @section Covariance:
#' To implement a covariance structure use Cholesky decomposition.
#' @family rng
#' @examplesIf pjrt::plugin_is_downloaded()
#' jit_eval({
#'   state <- nv_rng_state(42L)
#'   result <- nv_rnorm(c(2, 3), state)
#'   result[[2]]
#' })
#' @export
nv_rnorm <- function(shape, initial_state, dtype = "f32", mu = 0, sigma = 1) {
  checkmate::assertChoice(dtype, c("f32", "f64"))
  checkmate::assertNumeric(mu, len = 1, any.missing = FALSE)
  checkmate::assertNumeric(sigma, len = 1, any.missing = FALSE, lower = 0)
  shape <- assert_shapevec(shape)
  # n: amount of rvs needed
  n <- prod(shape)

  # Box-Muller Method:
  # from two random uniform variables u1 and u2 we can produce to normals z1, z2
  # z1 = sqrt(-2 * log(u1)) * cos(2 * pi * u2)
  # z2 = sqrt(-2 * log(u1)) * sin(2 * pi * u2)
  # Box-Muller works via polar representation of coordinates.
  # We scale this approach and genereate ceil(n/2) uniform rvs twice (U, Theta)

  # generate the first ceil(n/2) random uniform variables
  U <- nv_unif_rand(
    initial_state = initial_state,
    dtype = dtype,
    shape = as.integer(ceiling(n / 2))
  )

  # compute the radius R = sqrt(-2 * log(u1))
  R <- nv_mul(nv_log(U[[2]]), nv_scalar(-2, dtype = dtype))
  sqrt_R <- nv_sqrt(R)

  # generate second batch of ceil(n/2) random uniform variables
  Theta <- nv_unif_rand(initial_state = U[[1]], dtype = dtype, shape = as.integer(ceiling(n / 2)))

  # compute cos(2 * pi * u2) / sin(2 * pi * u2)
  Theta[[2]] <- nv_mul(Theta[[2]], nv_scalar(2 * pi, dtype = dtype))
  sin_Theta <- nv_sine(Theta[[2]])
  cos_Theta <- nv_cosine(Theta[[2]])

  # compute z1, z2
  Z1 <- nv_mul(sqrt_R, sin_Theta)
  Z2 <- nv_mul(sqrt_R, cos_Theta)

  # concatenate z = (z1, z2)
  Z <- nv_concatenate(Z1, Z2, dimension = 1L)

  # multiply with requested sd:
  # was:    var(Z) = 1
  # now:    var(Z) = sd^2
  N <- Z * sigma

  # add requested mu:
  # was:    mean(Z) = 0
  # now:    mean(Z) = mu
  N <- N + mu

  # if n is uneven, only keep N(1,...,n), i.e. discard last entry of N
  if (n %% 2 == 1) {
    N <- nv_static_slice(N, start_indices = 1L, limit_indices = n, strides = 1L)
  }

  # reshape N to match requested shape
  N <- nv_reshape(N, shape = shape)

  # return state and Normals N
  list(Theta[[1]], N)
}

#' @title Sample from a Binomial Distribution
#' @description
#' Samples from a binomial distribution with \eqn{n} trials and success probability \eqn{p}.
#' When `n = 1` (the default), this is a Bernoulli distribution.
#' @template param_shape
#' @template param_initial_state
#' @param n (`integer(1)`)\cr
#'   Number of trials.
#' @param prob (`numeric(1)`)\cr
#'   Probability of success on each trial.
#' @template param_dtype
#' @return (`list()` of [`tensorish`])\cr
#'   List of two elements: the updated RNG state and the sampled values.
#' @family rng
#' @examplesIf pjrt::plugin_is_downloaded()
#' jit_eval({
#'   state <- nv_rng_state(42L)
#'   # Bernoulli samples
#'   result <- nv_rbinom(c(2, 3), state)
#'   result[[2]]
#' })
#' @export
nv_rbinom <- function(shape, initial_state, n = 1L, prob = 0.5, dtype = "i32") {
  checkmate::assert_int(n, lower = 1)
  checkmate::assert_number(prob, lower = 0, upper = 1)
  shape <- assert_shapevec(shape)

  n_samples <- prod(shape)
  n_trials <- n_samples * n

  # Generate uniform samples in [0, 1) and compare to prob
  # Note that using runif() generates in (0, 1), but by shifting the 0 to the smallest value
  # so we don't benefit from using runif w.r.t. unbiasedness
  res <- nv_unif_rand(initial_state, shape = n_trials, dtype = "f64")
  U <- res[[2]]

  # Success if U < prob
  successes <- nv_convert(nv_lt(U, nv_scalar(prob, dtype = "f64")), dtype = dtype)

  result <- if (n == 1L) {
    nv_reshape(successes, shape = shape)
  } else {
    successes <- nv_reshape(successes, shape = c(n, shape))
    nv_reduce_sum(successes, dims = 1L, drop = TRUE)
  }

  list(res[[1]], result)
}

#' @title Sample from a Discrete Uniform Distribution
#' @description
#' Samples integers from `1` to `n` with equal probability (with replacement),
#' analogous to R's `sample.int(n, size, replace = TRUE)`.
#' @template param_shape
#' @template param_initial_state
#' @param n (`integer(1)`)\cr
#'   Number of categories (samples integers `1` to `n`).
#' @template param_dtype
#' @return (`list()` of [`tensorish`])\cr
#'   List of two elements: the updated RNG state and the sampled integers.
#' @family rng
#' @examplesIf pjrt::plugin_is_downloaded()
#' jit_eval({
#'   state <- nv_rng_state(42L)
#'   # Roll 6 dice
#'   result <- nv_rdunif(6, state, n = 6L)
#'   result[[2]]
#' })
#' @export
nv_rdunif <- function(shape, initial_state, n, dtype = "i32") {
  checkmate::assert_int(n, lower = 1)
  shape <- assert_shapevec(shape)
  n_sample <- prod(shape)

  # we sample uniformly and compute the maximial i, s.t. sum(bits[1:i]) <= F(x)

  # use f64 for higher precision
  res <- nv_unif_rand(initial_state, shape = n_sample, dtype = "f64")
  u <- res[[2]]

  cp <- nv_div(
    nv_add(nv_iota(1L, "f64", n), 1),
    nv_fill(n, "f64", shape = c())
  )

  u_col <- nv_reshape(u, c(n_sample, 1L))
  cp_row <- nv_reshape(cp, c(1L, n))
  bc <- nv_broadcast_tensors(u_col, cp_row) # (n_sample, n)
  lt_matrix <- nv_convert(nv_lt(bc[[2L]], bc[[1L]]), dtype = "i32")
  samples <- nv_add(nv_reduce_sum(lt_matrix, dims = 2L), 1L)

  return(list(res[[1]], nv_convert(nv_reshape(samples, shape), dtype)))
}
