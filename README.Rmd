---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# anvil

<!-- badges: start -->
[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental)
![R-CMD-check](https://github.com/r-xla/anvil/actions/workflows/R-CMD-check.yaml/badge.svg)
[![CRAN status](https://www.r-pkg.org/badges/version/anvil)](https://CRAN.R-project.org/package=anvil)
[![codecov](https://codecov.io/gh/r-xla/anvil/branch/main/graph/badge.svg)](https://codecov.io/gh/r-xla/anvil)
<!-- badges: end -->

Composable code transformation framework for R, allowing you to easily implement numerical programs efficiently.
It currently implements JIT compilation for very fast execution and backward-mode automatic differentiation.
Programs can run on various hardware backends, including CPU and GPU.

## Installation

``` r
pak::pak("r-xla/anvil")
```

## Quick Start

Below, we create a standard R function. We cannot directly call this function, but first need to wrap it in a `jit()` call.
If the resulting function is then called on `AnvilTensor`s -- the primary data type in {anvil} -- it will be JIT compiled and subsequently executed.

```{r}
library(anvil)
f <- function(a, b, x) {
  a * x + b
}
f_jit <- jit(f)

a <- nv_scalar(1.0)
b <- nv_scalar(-2.0)
x <- nv_scalar(3.0)

f_jit(a, b, x)
```

Through automatic differentiation, we can also obtain the gradient of `f`.

```{r}
g_jit <- jit(gradient(f, wrt = c("a", "b")))
g_jit(a, b, x)
```

## Main Features

- Automatic Differentiation:
  - Pullback for reverse mode automatic differentiation.
- Fast:
  - Code is JIT compiled into a single kernel.
  - Runs on various hardware, including CPU and GPU.
- Easy to extend and contribute:
  - Written almost entirely in R.
  - Easy to add new primitives and interpretation rules.

## Acknowledgments

- This work is supported by [MaRDI](https://www.mardi4nfdi.de).
- The design of this package was inspired by and borrows from:
  - JAX, especially the [autodidax tutorial](https://docs.jax.dev/en/latest/autodidax.html).
  - The [microjax](https://github.com/joey00072/microjax) project.
- For JIT compilation, we leverage the [OpenXLA](https://openxla.org/) project.
