---
title: "MCMC Sampling"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{MCMC Sampling}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

In this vignette, we implement the Metropolis-Hastings algorithm from scratch using {anvil}.
Markov Chain Monte Carlo (MCMC) methods are fundamental tools in Bayesian statistics for sampling from posterior distributions.
This example showcases {anvil}'s random number generation, JIT compilation, and loop constructs.

## The Problem

We consider a classic Bayesian inference problem: estimating the probability of success $\theta$ from binomial data.
Given $k$ successes out of $n$ trials, where $k \sim \text{Binomial}(n, \theta)$, and a Beta prior $\theta \sim \text{Beta}(\alpha, \beta)$, the posterior is:

$$\theta | k \sim \text{Beta}(\alpha + k, \beta + n - k)$$

While this posterior has an analytical solution (the Beta-Binomial is a conjugate model), we'll use it to validate our MCMC implementation.

## Simulating Data

Let's generate some observations from a binomial distribution with true success probability $\theta = 0.7$.

```{r}
library(anvil)
set.seed(123)

n_trials <- 50
true_theta <- 0.7

k_successes <- rbinom(1, size = n_trials, prob = true_theta)
k_successes
```

We specify a weakly informative prior $\theta \sim \text{Beta}(1, 1)$, which is equivalent to a uniform distribution on $[0, 1]$.

```{r}
alpha_prior <- 1
beta_prior <- 1
```

## The Log-Posterior

For numerical stability, we work with the log-posterior (up to a normalizing constant).
The log-posterior is the sum of the log-likelihood and the log-prior:

$$\log p(\theta | k) \propto k \log(\theta) + (n - k) \log(1 - \theta) + (\alpha - 1) \log(\theta) + (\beta - 1) \log(1 - \theta)$$

```{r}
log_posterior <- function(theta, k, n, alpha, beta) {
  # Log-likelihood: Binomial
  log_lik <- k * log(theta) + (n - k) * log(1 - theta)

  # Log-prior: Beta(alpha, beta)
  log_prior <- (alpha - 1) * log(theta) + (beta - 1) * log(1 - theta)

  log_lik + log_prior
}
```

## Metropolis-Hastings Algorithm

The Metropolis-Hastings algorithm generates samples from the posterior through an accept-reject mechanism:

1. Propose a new value $\theta'$ from a proposal distribution $q(\theta' | \theta)$
2. Compute the acceptance probability $\alpha = \min\left(1, \frac{p(\theta' | k) q(\theta | \theta')}{p(\theta | k) q(\theta' | \theta)}\right)$
3. Accept $\theta'$ with probability $\alpha$, otherwise keep $\theta$

We use a symmetric random walk proposal $\theta' \sim \mathcal{N}(\theta, \delta^2)$, which simplifies the acceptance ratio to the ratio of posteriors.
Since $\theta$ must be in $[0, 1]$, proposals outside this range are automatically rejected.

```{r}
mh_step <- function(theta, k, n, alpha, beta, proposal_sd, rng_state) {
  # Propose new value
  result <- nv_rnorm(1L, rng_state, dtype = "f32")
  rng_state <- result[[1]]
  z <- nv_reshape(result[[2]], shape = integer())  # reshape to scalar
  proposal <- theta + proposal_sd * z

  # Compute log acceptance ratio
  # Proposals outside [0, 1] get log_alpha = -Inf (automatic rejection)
  log_p_current <- log_posterior(theta, k, n, alpha, beta)
  log_p_proposal <- log_posterior(proposal, k, n, alpha, beta)
  log_alpha <- log_p_proposal - log_p_current

  # Reject if proposal is outside [0, 1]
  out_of_bounds <- (proposal <= 0) | (proposal >= 1)
  log_alpha <- nv_select(out_of_bounds, nv_scalar(-Inf, dtype = "f32"), log_alpha)

  # Accept or reject
  result <- nv_runif(1L, rng_state, dtype = "f32")
  rng_state <- result[[1]]
  u <- nv_reshape(result[[2]], shape = integer())  # reshape to scalar

  accepted <- log(u) < log_alpha
  theta_new <- nv_select(accepted, proposal, theta)

  list(theta = theta_new, rng_state = rng_state, accepted = accepted)
}
```

## Running the Sampler

We'll run the sampler for a number of iterations, storing the samples.
First, let's set up the data and parameters as tensors.

```{r}
k_tensor <- nv_scalar(k_successes, dtype = "f32")
n_tensor <- nv_scalar(n_trials, dtype = "f32")
alpha_tensor <- nv_scalar(alpha_prior, dtype = "f32")
beta_tensor <- nv_scalar(beta_prior, dtype = "f32")
proposal_sd <- nv_scalar(0.1, dtype = "f32")
```

We use `nv_while()` to run the entire sampling loop within a single compiled function.
This eliminates the overhead of switching between R and the XLA runtime on each iteration.
We store all samples in a tensor and update it at each iteration using `nvl_dynamic_update_slice()`.

```{r}
n_samples <- 5000L

mcmc_loop <- jit(function(k, n, alpha, beta, proposal_sd,
                          theta_init, rng_state) {
  samples <- nv_fill(0, shape = n_samples, dtype = "f32")

  result <- nv_while(
    list(
      theta = theta_init,
      samples = samples,
      rng_state = rng_state,
      i = nv_scalar(0L)
    ),
    \(theta, samples, rng_state, i) i < n_samples,
    \(theta, samples, rng_state, i) {
      step <- mh_step(theta, k, n, alpha, beta, proposal_sd, rng_state)

      # Store sample
      samples_new <- nvl_dynamic_update_slice(samples, nv_reshape(step$theta, 1L), i)

      list(
        theta = step$theta,
        samples = samples_new,
        rng_state = step$rng_state,
        i = i + 1L
      )
    }
  )

  result$samples
})
```

Now we can run the entire MCMC chain with a single function call.

```{r}
samples <- as_array(mcmc_loop(
  k_tensor, n_tensor, alpha_tensor, beta_tensor, proposal_sd,
  nv_scalar(0.5, dtype = "f32"),
  nv_rng_state(seed = 42L)
))
```

## Results

Let's examine the trace plot to check for convergence.

```{r, echo = FALSE, fig.width = 6, fig.height = 4}
plot(samples, type = "l", col = "steelblue",
     xlab = "Iteration", ylab = expression(theta),
     main = "Trace Plot")
```

After a short burn-in period, the chain appears to have converged.
Let's discard the first 500 samples as burn-in and examine the posterior distribution.

```{r}
burn_in <- 500
posterior_samples <- samples[(burn_in + 1):n_samples]
```

```{r, echo = FALSE, fig.width = 6, fig.height = 4}
hist(posterior_samples, breaks = 40, freq = FALSE, col = "lightblue",
     xlab = expression(theta), main = "Posterior Distribution",
     xlim = c(0.4, 1))

# Overlay analytical posterior: Beta(alpha + k, beta + n - k)
alpha_post <- alpha_prior + k_successes
beta_post <- beta_prior + n_trials - k_successes
curve(dbeta(x, alpha_post, beta_post), add = TRUE, col = "red", lwd = 2)
legend("topleft", legend = c("MCMC samples", "Analytical posterior"),
       fill = c("lightblue", NA), border = c("black", NA),
       lty = c(NA, 1), col = c(NA, "red"), lwd = c(NA, 2))
```

The MCMC samples match the analytical posterior well.
Let's compare the posterior summaries.

```{r}
# Analytical posterior moments
posterior_mean <- alpha_post / (alpha_post + beta_post)
posterior_var <- (alpha_post * beta_post) / ((alpha_post + beta_post)^2 * (alpha_post + beta_post + 1))

cat("MCMC posterior mean:", mean(posterior_samples), "\n")
cat("Analytical posterior mean:", posterior_mean, "\n")
cat("MCMC posterior sd:", sd(posterior_samples), "\n")
cat("Analytical posterior sd:", sqrt(posterior_var), "\n")
```
