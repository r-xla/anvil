---
title: "Autodiff Modes"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Autodiff Modes}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

In this vignette, we will explore the different modes of autodiff.

Let's say, we have a function

$$
\newcommand{\R}{\mathbb{R}}
$$

$$ f(b) = a \times b^\top \times x $$

where $a \, \in \, \R^{m \times 1},\, b \, \in \, \R^{1 \times p},\, x \, \in \, \R^{p \times 1}$

We can write this as:

$$
\begin{bmatrix}
y_1 \\
\vdots \\
y_m
\end{bmatrix}
= \begin{bmatrix}
a_1 \\
\vdots \\
a_m
\end{bmatrix} \times
\begin{bmatrix}
b_1, \ldots, b_p
\end{bmatrix} \times
\begin{bmatrix}
x_1 \\
\vdots \\
x_p
\end{bmatrix}
$$

Note that the specific form of the function $f$ is not important.
The dimensions are chosen in such a way, that it's apparent what the advantage and disadvantage of forward and reverse mode autodiff is.

The jacobian of $f$ with respect to $b$ is:

$$
\begin{bmatrix}
\frac{\partial y_1}{\partial b_1} & \cdots & \frac{\partial y_1}{\partial b_p} \\
\vdots & \ddots & \vdots \\
\frac{\partial y_m}{\partial b_1} & \cdots & \frac{\partial y_m}{\partial b_p}
\end{bmatrix}
$$

If we denote the temporary variables of $b^\top \times x$ as $z$, we can use the chain rule to obtain the formula:

$$
\frac{\partial f}{\partial b} = \frac{\partial f}{\partial z} \times \frac{\partial z}{\partial b}
$$

Writing out the matrices, we get:

$$
\begin{bmatrix}
\frac{\partial y_1}{\partial b_1} & \cdots & \frac{\partial y_1}{\partial b_p} \\
\vdots & \ddots & \vdots \\
\frac{\partial y_m}{\partial b_1} & \cdots & \frac{\partial y_m}{\partial b_p}
\end{bmatrix}
= \begin{bmatrix}
\frac{\partial y_1}{\partial z_1} & \cdots & \frac{\partial y_1}{\partial z_l} \\
\vdots & \ddots & \vdots \\
\frac{\partial y_m}{\partial z_1} & \cdots & \frac{\partial y_m}{\partial z_l}
\end{bmatrix} \times
\begin{bmatrix}
\frac{\partial z_1}{\partial b_1} & \cdots & \frac{\partial z_1}{\partial b_p} \\
\vdots & \ddots & \vdots \\
\frac{\partial z_l}{\partial b_1} & \cdots & \frac{\partial z_l}{\partial b_p}
\end{bmatrix}
$$

The question is now, how we can actually **calculate** this computationally.
For computational feasibility, we want to avoid calculating the full intermediate matrices, as these can be very large.

To ways to do this are:

- Forward mode autodiff
- Reverse mode autodiff

### Forward mode autodiff

In this case, we build up the jacobian column by column.
I.e., we calculate the partial derivative of $f$ with respect to $b_1$, then $b_2$, etc.

In practice, we start by multiplying $\frac{\partial z}{\partial b}$  with the one hot vector $e_1$ (i.e., the first column of the identity matrix).
Writing this out with matrices, we get:

$$
\begin{bmatrix}
\frac{\partial z_1}{\partial b_1} & \cdots & \frac{\partial z_1}{\partial b_p} \\
\vdots & \ddots & \vdots \\
\frac{\partial z_l}{\partial b_1} & \cdots & \frac{\partial z_l}{\partial b_p}
\end{bmatrix}\times
\begin{bmatrix}
1 \\
\vdots \\
0
\end{bmatrix}=
\begin{bmatrix}
\frac{\partial z_1}{\partial b_1} \\
\vdots \\
\frac{\partial z_l}{\partial b_1}
\end{bmatrix}
$$

This will give us an intermediate matrix $u \in \R^{l \times 1}$.

To get the final result, we need to multiply the jacobian of $y$ with respect to $z$ with $u$.

$$
\begin{bmatrix}
\frac{\partial y_1}{\partial b_1} \\
\vdots \\
\frac{\partial y_m}{\partial b_1}
\end{bmatrix}
=
\begin{bmatrix}
\frac{\partial y_1}{\partial z_1} & \cdots & \frac{\partial y_1}{\partial z_l} \\
\vdots & \ddots & \vdots \\
\frac{\partial y_m}{\partial z_1} & \cdots & \frac{\partial y_m}{\partial z_l}
\end{bmatrix} \times \begin{bmatrix}
u_1 \\ \vdots \\ u_l
\end{bmatrix}
$$

Note that we could -- mathematically -- of course multiply not only with a single unity vector, but with the whole identity matrix.
The reason this is not done, is because of the memory costs and computational complexity.

In the first step of the forward mode AD we multiply a matrix of size $l \times p$ with a matrix of size $p \times 1$.
and the result is a matrix of size $l \times 1$.
But if we would multiply with the identity matrix, we would start out with a matrix of size $p \times p$ and the first intermediate result would be a matrix of size $l \times p$.

So, we is this an issue, even with a single unity vector we have an intermediate jacobian of size $l \times p$, so why is another one of size $p \times p$ an issue?

The reason is, that the jacobian of size $l \times p$ is never stored in memory, whereas the intermediate value of size $p \times p$ would have to be stored.

### Reverse mode autodiff

Instead of building up the jacobian column by column, we build up the jacobian row by row.
I.e., we calculate the partial derivative of $f$ with respect to $b_1$, then $b_2$, etc.

We go from forward mode to reverse mode by transposing the jacobian of $f$ with respect to $z$.

$$
\frac{\partial f}{\partial b}^\top =
\big[\frac{\partial f}{\partial z} \times \frac{\partial z}{\partial b}\big]^\top=
\frac{\partial z}{\partial b}^\top \times \frac{\partial f}{\partial z}^\top
$$

If we now multiply this with a unity vector $r_i \in \R^{1 \times m}$, which determines with respect to which output variable we calculate the jacobian, we get:

$$
\frac{\partial f}{\partial b_1}^\top \times r_1 = \frac{\partial z}{\partial b_1}^\top \times \frac{\partial f}{\partial z}^\top \times r_1
$$

In full matrix notation, we get:

$$
\begin{bmatrix}
\frac{\partial y_1}{\partial b_1} \\
\vdots \\
\frac{\partial y_1}{\partial b_p}
\end{bmatrix} =
\begin{bmatrix}
\frac{\partial z_1}{\partial b_1} & \cdots & \frac{\partial z_l}{\partial b_1} \\
\vdots & \ddots & \vdots \\
\frac{\partial z_1}{\partial b_p} & \cdots & \frac{\partial z_l}{\partial b_p}
\end{bmatrix} \times
\begin{bmatrix}
\frac{\partial y_1}{\partial z_1} & \cdots & \frac{\partial y_m}{\partial z_1} \\
\vdots & \ddots & \vdots \\
\frac{\partial y_1}{\partial z_l} & \cdots & \frac{\partial y_m}{\partial z_l}
\end{bmatrix} \times
\begin{bmatrix}
1 \\
0 \\
\vdots \\
0
\end{bmatrix}
$$

If we look closely at the last equation, we see now, that this requires us to first calculate a forward pass, before we calculate the backward pass.
This is, because we want to start with the rightmost multiplication, as this reduces the computational complexity.
But the rightmost multiplication involves the derivative of $f$ with respect to the temporary variables $z$.
Therefore, we first need to obtain these values using a forward pass.

Now, for the first step, we have computational complexity $O(m \times l)$ and for the second one $O(p \times l)$.
