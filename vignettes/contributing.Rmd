---
title: "Implementing a New Primitive"
format: html
vignette: >
  %\VignetteIndexEntry{Implementing a New Primitive}
  %\VignetteEngine{quarto::html}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This guide explains how to implement a new primitive in {anvil}.

## What is a Primitive?

Primitives (`nvl_*` functions) are the atomic building blocks of {anvil}.
Understanding what a primitive is and does is essential for contributing new functionality.

### Primitives as Graph-Building Functions

First and foremost, a primitive is a **graph-building function**.
When you call a primitive like `nvl_add(x, y)` during tracing (i.e., within a `jit()`-compiled function or `trace_fn()`), it does *not* immediately compute a result.
Instead, it:
1. Creates a `PrimitiveCall` that records the operation
2. Attaches this call to the current `GraphDescriptor`
3. Returns `GraphValue` objects representing the (future) outputs

This is how {anvil} builds up a computational graph that can later be transformed (e.g., differentiated) or lowered to an executable.

```{r}
library(anvil)

# During tracing, primitives build a graph
graph <- trace_fn(function(x, y) {
  nvl_add(x, y)
}, list(x = nv_aten("f32", c()), y = nv_aten("f32", c())))
graph
```

The graph contains:

- **inputs**: `GraphNode`s representing function arguments
- **outputs**: `GraphNode`s representing return values
- **calls**: `PrimitiveCall`s recording each primitive invocation

### Interpretation Rules

The graph itself is just a data structure -- it doesn't *do* anything until we apply a **transformation**.
Each primitive can have multiple interpretation rules stored in its `$rules` field:

- `"stablehlo"`: How to lower this primitive to StableHLO operations (for JIT compilation)
- `"backward"`: How to compute gradients through this primitive (for automatic differentiation)

These rules are only relevant when transforming the graph, not when building it:

```{r}
# The stablehlo rule for addition
prim("add")$rules[["stablehlo"]]

# The backward rule for addition
prim("add")$rules[["backward"]]
```

### Inputs vs Parameters

A `PrimitiveCall` distinguishes between two types of arguments:

- **Inputs** (`args`): Tensor values that flow through the graph. These are dynamic and become `GraphNode`s. Their values are only known at runtime.
- **Parameters** (`params`): Static R values known at compile time (shapes, dimension indices, algorithm choices, etc.). These are baked into the compiled executable.

For example, in `nvl_broadcast_in_dim(operand, shape, broadcast_dimensions)`:

- `operand` is an **input** (a tensor)
- `shape` and `broadcast_dimensions` are **parameters** (integer vectors known at compile time)

This distinction matters because parameters can influence the structure of the generated code, while inputs only affect the values flowing through that structure.

## Why Create a New Primitive?

Before implementing a new primitive, consider whether it's necessary.
There are two main reasons to create one:

### 1. Functionality Not Expressible with Existing Primitives

Some operations simply cannot be built from existing primitives.
For example, random number generation requires special hardware support and cannot be decomposed into arithmetic operations.

### 2. Custom Derivatives or Efficiency

Sometimes an operation *can* be expressed using existing primitives (e.g., as a composition or a `nv_while` loop), but:

- **You want a custom derivative**: The automatic differentiation of a `nv_while` loop may not be what you want, or may not even be possible for your use case. A dedicated primitive lets you define exactly how gradients should flow.

- **More efficient gradients**: The chain rule applied naively might produce suboptimal gradient computations. A dedicated primitive can implement a more efficient backward pass.

For example, `reduce_sum` could theoretically be implemented as a while loop, but:

1. The backward pass for a generic while loop is complex
2. The gradient of a sum is simply broadcasting, which is much simpler and more efficient than what automatic differentiation of a while loop would produce

## The Relationship to StableHLO

[StableHLO](https://github.com/openxla/stablehlo) is a portable intermediate representation for machine learning computations.
It serves as the bridge between {anvil} and the XLA compiler.

The {stablehlo} R package provides:

- `hlo_*` functions for building StableHLO operations
- `infer_types_*` functions for type inference

When you lower an `AnvilGraph` to StableHLO via `stablehlo()`, each `PrimitiveCall` is translated using its `"stablehlo"` rule.

### Indexing Convention

**Important**: StableHLO uses **0-based indexing**, while {anvil} uses R's **1-based indexing**.
When implementing a StableHLO rule, you must convert indices by subtracting 1:

```{r, eval = FALSE}
# In a stablehlo rule, convert 1-based to 0-based
p_transpose[["stablehlo"]] <- function(operand, permutation) {
  list(stablehlo::hlo_transpose(operand, permutation - 1L))  # Note: - 1L
}
```

## Implementing a New Primitive

Let's walk through the steps to implement a new primitive.
We'll use the `abs` (absolute value) primitive as a concrete example.

### Step 1: Create the AnvilPrimitive

First, define the primitive object in `primitives.R`.
This registers the primitive with the system.

```{r, eval = FALSE}
p_abs <- AnvilPrimitive("abs")
```

### Step 2: Define the Primitive Function

Next, implement the `nvl_*` function.
This function must:

1. Define an **inference function** that computes output types from input types
2. Call `graph_desc_add()` to record the operation in the graph

```{r, eval = FALSE}
#' @title Primitive Absolute Value
#' @description Element-wise absolute value.
#' @param operand A tensor.
#' @return A tensor of the same shape and dtype.
#' @export
nvl_abs <- function(operand) {
  infer_fn <- function(operand) {
    # Use stablehlo's type inference
    out <- stablehlo::infer_types_abs(at2vt(operand))[[1L]]
    out <- vt2at(out)
    # Propagate ambiguity from input to output
    out$ambiguous <- operand$ambiguous
    list(out)
  }

  graph_desc_add(
    p_abs,                      # The primitive
    list(operand = operand),    # Inputs (tensors)
    params = list(),            # Parameters (static values) - none for abs
    infer_fn = infer_fn
  )[[1L]]
}
```

Key points:

- `at2vt()` converts an `AbstractTensor` to a `ValueType` (stablehlo's type representation)
- `vt2at()` converts back
- Always propagate `ambiguous` from inputs to outputs appropriately
- `graph_desc_add()` returns a list of outputs; extract with `[[1L]]` for single-output primitives

For common patterns, helper functions exist:

```{r, eval = FALSE}
# For simple unary operations:
nvl_abs <- make_unary_op(p_abs, stablehlo::infer_types_abs)

# For simple binary operations:
nvl_add <- make_binary_op(p_add, stablehlo::infer_types_add)
```

### Step 3: Add the StableHLO Rule

In `rules-stablehlo.R`, define how to lower this primitive to StableHLO:

```{r, eval = FALSE}
p_abs[["stablehlo"]] <- function(operand) {
  list(stablehlo::hlo_abs(operand))
}
```

The rule receives the same arguments as the primitive function (both inputs and params).
It must return a list of StableHLO values corresponding to the outputs.

### Step 4: Add the Backward Rule (Optional)

If the operation should support automatic differentiation, add a backward rule in `rules-backward.R`:

```{r, eval = FALSE}
p_abs[["backward"]] <- function(inputs, outputs, grads, .required) {
  operand <- inputs[[1L]]
  grad <- grads[[1L]]
  list(
    # d/dx |x| = sign(x)
    if (.required[[1L]]) nvl_mul(grad, nvl_sign(operand))
  )
}
```

The backward rule receives:

- `inputs`: List of input `GraphValue`s from the forward pass
- `outputs`: List of output `GraphValue`s from the forward pass
- `grads`: Gradients flowing back from downstream (one per output)
- `.required`: Logical vector indicating which input gradients are actually needed

It must return a list of gradients with respect to each input (or `NULL` if not required).

### Step 5: Add Tests

For primitives, tests can go in one of two places:

1. **`inst/extra-tests/`**: For tests that compare against torch. These files live in `inst/` rather than `tests/` to avoid listing torch as a `Suggests` dependency (torch is large and not available on all platforms). The files `test-primitives-stablehlo-torch.R` and `test-primitives-backward-torch.R` are sourced by the main test files when torch is available.

2. **`tests/testthat/`**: For tests without a torch counterpart, add them directly to `test-primitives-stablehlo.R` or `test-primitives-backward.R`.

**Important**: Test names must start with `"p_<name>"` (matching the primitive object name).
The meta tests in `test-primitives-meta.R` verify that every primitive has corresponding tests.
If you name your test differently, the CI will fail.

When a torch equivalent exists, prefer comparing against it:

```{r, eval = FALSE}
# In inst/extra-tests/test-primitives-stablehlo-torch.R
test_that("p_abs matches torch", {
  x <- nv_tensor(c(-1, 2, -3))
  result <- jit(nvl_abs)(x)
  expected <- torch::torch_abs(torch::torch_tensor(c(-1, 2, -3)))
  expect_equal(as_array(result), as.array(expected))
})

# In inst/extra-tests/test-primitives-backward-torch.R
test_that("p_abs backward matches torch", {
  # ... test gradient computation against torch
})
```

When no torch equivalent exists, test directly:

```{r, eval = FALSE}
# In tests/testthat/test-primitives-stablehlo.R
test_that("p_my_custom_op works", {
  x <- nv_tensor(c(1, 2, 3))
  result <- jit(nvl_my_custom_op)(x)
  expect_equal(as_array(result), c(...))  # expected values
})
```

## Higher-Order Primitives

Some primitives contain **subgraphs** -- nested computations that are traced separately.
Examples include `nv_if` and `nv_while`.

```{r, eval = FALSE}
# A higher-order primitive declares its subgraph parameters
p_while <- AnvilPrimitive("while", subgraphs = c("cond_graph", "body_graph"))
```

Implementing higher-order primitives is more complex and requires careful handling of graph contexts.
See the implementation of `nvl_while` in `primitives.R` for a detailed example.

## Summary Checklist

When implementing a new primitive:

- [ ] Create `AnvilPrimitive` object in `primitives.R`
- [ ] Implement `nvl_*` function with proper type inference
- [ ] Add `"stablehlo"` rule in `rules-stablehlo.R` (remember: 0-based indexing!)
- [ ] Add `"backward"` rule in `rules-backward.R` if differentiable
- [ ] Add documentation with roxygen2
- [ ] Add tests (ideally comparing with torch)
- [ ] Export the function in `NAMESPACE`
